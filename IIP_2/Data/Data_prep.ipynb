{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Assessment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Vorhersage der Personen-Dichte ist ein wichtiger Aspekt für die Stadtplanung und die Verkehrsplanung. Um genaue Vorhersagen treffen zu können, ist es wichtig, dass die zugrundeliegenden Daten eine hohe Qualität aufweisen. In diesem Data Quality Assessment werden die Daten vom Bundesamt für Statistik (BFS) untersucht, die für die Vorhersage der Personen-Dichte verwendet werden. Das Ziel dieses Assessments ist es, die Qualität der Daten zu bewerten und mögliche Fehler oder Unregelmäßigkeiten zu identifizieren. Eine gründliche Analyse der Daten ist unerlässlich, um sicherzustellen, dass die Vorhersagen auf einer soliden Grundlage basieren und somit verlässliche Entscheidungen für die Stadt- und Verkehrsplanung getroffen werden können.\n",
    "\n",
    "### Daten\n",
    "Im Rahmen dieses Data Quality Assessments werden folgende Datensätze vom BFS analysiert:\n",
    "\n",
    "1. Arbeitsstätten und Beschäftigte nach Gemeinde und Wirtschaftssektor:\n",
    "    -  Dataset: [Arbeitsstätten und Beschäftigte](https://www.bfs.admin.ch/asset/de/23284713)\n",
    "\n",
    "    - Beschreibung: Dieser Datensatz enthält jährliche Zahlen der Arbeitsstätten und Beschäftigten nach Gemeinde und Wirtschaftssektor ab dem Jahr 2011\n",
    "\n",
    "2. Bauausgaben und Arbeitsvorrat nach Grossregion, Kanton, Gemeinde, Art der Auftraggeber, Art der Bauwerke und Art der Arbeiten:\n",
    "\n",
    "    - Dataset: [Bauausgaben und Arbeitsvorrat](https://www.bfs.admin.ch/asset/de/22907406)\n",
    "\n",
    "    - Beschreibung: Dieser Datensatz präsentiert jährliche Bauausgaben und den Arbeitsvorrat nach Grossregion, Kanton, Gemeinde, Art der Auftraggeber, Art der Bauwerke und Art der Arbeiten ab 1994.\n",
    "\n",
    "3. Bewohnte Wohnungen nach Gemeinde, Anzahl Zimmer und Bewohnertyp, 1990 und 2000:\n",
    "\n",
    "    - Dataset: [Bewohnte Wohnungen](https://www.bfs.admin.ch/asset/de/283778)\n",
    "\n",
    "    - Beschreibung: Dieser Datensatz enthält Informationen zur Anzahl bewohnter Wohnungen nach Gemeinde, Anzahl Zimmer und Bewohnertyp (Mieter/in, Genossenschafter/in, Inhaber/in, Pächter/in) für die Jahre 1990 und 2000.\n",
    "\n",
    "4. Regionalporträts: Kenzahlen aller Gemeinden:\n",
    "\n",
    "    - Dataset: [Regionalporträts](https://www.bfs.admin.ch/bfs/de/home/statistiken/regionalstatistik/regionale-portraets-kennzahlen/gemeinden.html)\n",
    "\n",
    "    - Beschreibung: Diese Datensets bieten eine Vielzahl von Kennzahlen für alle Gemeinden, darunter Bevölkerung, Altersverteilung, Bevölkerungsbewegungen, Haushalte, Fläche, Wirtschaft, Bau- und Wohnungswesen, Soziale Sicherheit und Wähleranteil der Parteien (Nationalratswahlen) ab dem Jahr 2013\n",
    "\n",
    "5. Amtliches Gemeindeverzeichnis der Schweiz:\n",
    "\n",
    "    - Dataset: [Amtliches Gemeindeverzeichnis](https://www.bfs.admin.ch/bfs/de/home/grundlagen/agvch.html)\n",
    "\n",
    "    - Beschreibung: Dieses Verzeichnis ist nach Kantonen sowie nach Bezirken oder einer vergleichbaren administrativen Einheit des Kantons gegliedert und bietet Informationen zu den Gemeinden.\n",
    "\n",
    "\n",
    "### Qualitätsbewertung\n",
    "\n",
    "Bei der Bewertung der Datenqualität werden verschiedene Aspekte berücksichtigt:\n",
    "\n",
    "- Vollständigkeit: Es wird überprüft, ob alle relevanten Daten vorhanden sind und keine wichtigen Informationen fehlen.\n",
    "\n",
    "- Genauigkeit: Es wird die Genauigkeit der Daten überprüft, um sicherzustellen, dass sie korrekt und präzise sind.\n",
    "\n",
    "- Aktualität: Es wird überprüft, ob die Daten auf dem neuesten Stand sind und den aktuellen Zeitraum abdecken.\n",
    "\n",
    "- Konsistenz: Es wird die Konsistenz der Daten über verschiedene Quellen und Datensätze hinweg überprüft, um sicherzustellen, dass sie miteinander vereinbar sind.\n",
    "\n",
    "\n",
    "Durch eine gründliche Analyse der Daten und die Bewertung dieser Qualitätsaspekte können mögliche Fehler oder Unregelmäßigkeiten identifiziert werden. Dies ermöglicht es, etwaige Datenprobleme zu beheben oder zu korrigieren und somit verlässliche Daten für die Vorhersage der Personen-Dichte zu gewährleisten.\n",
    "\n",
    "### Python Bibliotheken\n",
    "\n",
    "Für die Verwendung des Notebooks müssen die folgenden Bibliotheken installiert sein.\n",
    "\n",
    "#### NumPy\n",
    "\n",
    "[NumPy](https://numpy.org) ist eine numerische Bibliothek für die Bearbeitung von Matrizen und Arrays.  \n",
    "NumPy wurde hauptsächlich implizit verwendet, da viele andere Bibliotheken mit NumPy Arrays arbeiten.\n",
    "\n",
    "#### Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) ist eine Bibliothek für Datenverarbeitung und erlaubt das effiziente Einlesen und Manipulieren von Daten. Pandas baut auf NumPy auf.\n",
    "\n",
    "#### PyAxis\n",
    "[PyAxis](https://github.com/icane/pyaxis) ist eine Python-Bibliothek zur Manipulation von PC-Axis (oder PX) formatierten Daten. Sie ermög-licht das Lesen und Schreiben von PC-Axis-Formaten mit Python unter Verwendung der von der weit ver-breiteten pandas-Bibliothek bereitgestellten DataFrame-Strukturen. PX ist ein Standardformat für statis-tische Dateien, das von einer Vielzahl statistischer Ämter verwendet wird.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassen für Datenverarbeitung und Datenqualitätsbewertung\n",
    "\n",
    "In diesem Abschnitt werden fünf Klassen vorgestellt, die für die Verarbeitung und Qualitätsbewertung von Daten entwickelt wurden. Die Klassen sind:\n",
    "\n",
    "- `MultindexDf`: Diese Klasse ermöglicht die Erstellung eines DataFrames mit mehrstufigem Index, um komplexe Datenstrukturen zu unterstützen.\n",
    "\n",
    "- `DataFrameMerger`: Diese Klasse dient zum Zusammenführen mehrerer DataFrames.\n",
    "\n",
    "- `ExcelProcessor`: Diese Klasse verarbeitet Excel-Daten, erstellt mehrstufige Header und verbindet Daten für jedes Jahr.\n",
    "\n",
    "- `PXProcessor`: Diese Klasse verarbeitet `.px`-Dateien mit Hilfe der `pyaxis` Bibliothek und extrahiert relevante Daten und organisiert sie in einem DataFrame.\n",
    "\n",
    "- `DataQualityAssessment`: Diese Klasse überprüft die Datenqualität, indem sie fehlende Werte, Datentypen und Duplikate prüft und identifiziert Duplikate.\n",
    "\n",
    "---\n",
    "\n",
    "Dies sind die Klassen, die entwickelt wurden, um die Datenverarbeitung und die Bewertung der Datenqualität zu unterstützen. Jede Klasse erfüllt eine spezifische Funktion und kann in verschiedenen Szenarien zur Datenanalyse und -verarbeitung eingesetzt werden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultindexDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultindexDf` ist eine Klasse, die verschiedene Methoden zur Arbeit mit DataFrames mit Multiindex bereitstellt.<br>\n",
    "Die Methoden ermöglichen das Erstellen eines Multiheaders für die Spalten, das Erstellen eines Multiindex für die Zeilen,<br>\n",
    "das Setzen der Schlüssel als Multiheader, das Filtern von Spalten basierend auf Titeln oder Untertiteln,<br>\n",
    "das Lesen von Excel-Dateien mit Multiindex und das Entfernen von doppelten Indizes im DataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methoden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `create_multiheader(df, header_list_1, header_list_2)`\n",
    "Erstellt einen Multiheader für die Spalten des DataFrames basierend auf zwei gegebenen Header-Listen.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, für den der Multiheader erstellt werden soll.\n",
    "- `header_list_1 (list)`: Eine Liste von Strings für den ersten Teil des Multiheaders.\n",
    "- `header_list_2 (list)`: Eine Liste von Strings für den zweiten Teil des Multiheaders.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit dem erstellten Multiheader für die Spalten.\n",
    "\n",
    "#### `create_multiindex(df, index_columns)`\n",
    "Erstellt einen Multiindex für den DataFrame basierend auf den angegebenen Indexspalten.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, für den der Multiindex erstellt werden soll.\n",
    "- `index_columns (list)`: Eine Liste von Strings, die die Namen der Indexspalten enthalten.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit dem erstellten Multiindex.\n",
    "\n",
    "#### `set_keys_as_multiheader(df_dict)`\n",
    "Setzt die Schlüssel des gegebenen Dictionarys als Multiheader für die entsprechenden DataFrames.\n",
    "\n",
    "- `df_dict (dict)`: Ein Dictionary, das die DataFrames enthält, für die der Multiheader gesetzt werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das aktualisierte Dictionary mit den DataFrames, bei denen der Multiheader gesetzt wurde.\n",
    "\n",
    "#### `find_subheaders(df, header_name)`\n",
    "Sucht alle Untertitel (subheaders) in einem DataFrame, die dem angegebenen Headernamen entsprechen.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, in dem nach den Untertiteln gesucht werden soll.\n",
    "- `header_name (str)`: Der Name des Headers, für den die Untertitel gefunden werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `list`: Eine Liste der Untertitel, die dem angegebenen Headernamen entsprechen.\n",
    "\n",
    "#### `df_has_Multindex(df)`\n",
    "Überprüft, ob der DataFrame einen Multiindex für die Spalten hat.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, der überprüft werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `bool`: True, wenn der DataFrame einen Multiindex hat, andernfalls False.\n",
    "\n",
    "#### `filter_by_titles(df, titles, level)`\n",
    "Filtert den DataFrame basierend auf den gewünschten Titeln für einen bestimmten Level des Multiheaders.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, der gefiltert werden soll.\n",
    "- `titles (list)`: Eine Liste von Strings, die die gewünschten Titel für den Filter enthalten.\n",
    "- `level (int)`: Der Level des Multiheaders, auf dem der Filter angewendet werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der gefilterte DataFrame, der nur die Spalten mit den gewünschten Titeln enthält.\n",
    "\n",
    "#### `filter_by_titles_or_subtitel(df, titles=[], subtitles=[], multheader=True)`\n",
    "Filtert den DataFrame basierend auf den gewünschten Titeln oder Untertiteln (subtitles) des Multiheaders.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, der gefiltert werden soll.\n",
    "- `titles (list)`: Eine Liste von Strings, die die gewünschten Titel für den Filter enthalten.\n",
    "- `subtitles (list)`: Eine Liste von Strings, die die gewünschten Untertitel für den Filter enthalten.\n",
    "- `multheader (bool)`: Gibt an, ob der gefilterte DataFrame den Multiheader beibehalten soll. True, wenn der Multiheader beibehalten werden soll, False, wenn der Multiheader entfernt werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der gefilterte DataFrame, der nur die Spalten mit den gewünschten Titeln oder Untertiteln enthält. Der Multiheader kann optional beibehalten oder entfernt werden.\n",
    "\n",
    "#### `read_multiindex_excel(path_pre, file_name)`\n",
    "Liest eine Excel-Datei mit einem Multiindex und gibt den entsprechenden DataFrame zurück.\n",
    "\n",
    "- `path_pre (str)`: Der Pfad zur Excel-Datei.\n",
    "- `file_name (str)`: Der Name der Excel-Datei.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame, der aus der Excel-Datei mit einem Multiindex gelesen wurde.\n",
    "\n",
    "#### `drop_duplicated(df)`\n",
    "Entfernt doppelte Indizes (Zeilen) im DataFrame und gibt den bereinigten DataFrame zurück.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, bei dem doppelte Indizes entfernt werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der bereinigte DataFrame ohne doppelte Indizes.\n",
    "\n",
    "Die Klasse bietet eine Reihe nützlicher Funktionen für die Arbeit mit DataFrames mit Multiindex und kann in verschiedenen Datenanalyse- oder Data-Science-Projekten eingesetzt werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultindexDf():    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_multiheader(self, df, header_list_1, header_list_2):\n",
    "        \"\"\"\n",
    "        Erstellt einen Multiheader für die Spalten des DataFrames basierend auf zwei gegebenen Header-Listen.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, für den der Multiheader erstellt werden soll.\n",
    "            header_list_1 (list): Eine Liste von Strings für den ersten Teil des Multiheaders.\n",
    "            header_list_2 (list): Eine Liste von Strings für den zweiten Teil des Multiheaders.\n",
    "        Returns:\n",
    "            DataFrame: Der DataFrame mit dem erstellten Multiheader für die Spalten.\n",
    "        \"\"\"\n",
    "        multi_header = list(zip(header_list_1, header_list_2))\n",
    "        multi_index = pd.MultiIndex.from_tuples(multi_header)\n",
    "        df.columns = multi_index\n",
    "        return df\n",
    "    \n",
    "    def create_multiindex(self,df, index_columns):        \n",
    "        \"\"\"\n",
    "        Erstellt einen Multiindex für den DataFrame basierend auf den angegebenen Indexspalten.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, für den der Multiindex erstellt werden soll.\n",
    "            index_columns (list): Eine Liste von Strings, die die Namen der Indexspalten enthalten.\n",
    "        Returns:\n",
    "            DataFrame: Der DataFrame mit dem erstellten Multiindex.\n",
    "        \"\"\"\n",
    "        multiindexed = df.set_index(index_columns)      \n",
    "        return multiindexed\n",
    "\n",
    "    def set_keys_as_multiheader(self,df_dict):\n",
    "        \"\"\"\n",
    "        Setzt die Schlüssel des gegebenen Dictionarys als Multiheader für die entsprechenden DataFrames.\n",
    "        Args:\n",
    "            df_dict (dict): Ein Dictionary, das die DataFrames enthält, für die der Multiheader gesetzt werden soll.\n",
    "        Returns:\n",
    "            dict: Das aktualisierte Dictionary mit den DataFrames, bei denen der Multiheader gesetzt wurde.\n",
    "        \"\"\" \n",
    "        for name, df in df_dict.items():\n",
    "            if self.df_has_Multindex(df):\n",
    "                break\n",
    "            df = self.create_multiheader(df,[name]*len(df.columns),df.columns.to_list())\n",
    "            df_dict[name] = df\n",
    "        return df_dict\n",
    "    \n",
    "    def find_subheaders(self, df, header_name):\n",
    "        \"\"\"\n",
    "        Sucht alle Untertitel (subheaders) in einem DataFrame, die dem angegebenen Headernamen entsprechen.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, in dem nach den Untertiteln gesucht werden soll.\n",
    "            header_name (str): Der Name des Headers, für den die Untertitel gefunden werden sollen.\n",
    "        Returns:\n",
    "            list: Eine Liste der Untertitel, die dem angegebenen Headernamen entsprechen.\n",
    "        \"\"\"\n",
    "        headers = df.columns.values\n",
    "        subheaders = []\n",
    "        for header in headers:\n",
    "            if header[0] == header_name:\n",
    "                subheaders.append(header[1])\n",
    "        return subheaders\n",
    "    \n",
    "    def df_has_Multindex(self,df):\n",
    "        \"\"\"\n",
    "        Überprüft, ob der DataFrame einen Multiindex für die Spalten hat.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, der überprüft werden soll.\n",
    "        Returns:\n",
    "            bool: True, wenn der DataFrame einen Multiindex hat, andernfalls False.\n",
    "        \"\"\"\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def filter_by_titles(self,df, titles,level):\n",
    "        \"\"\"\n",
    "        Filtert den DataFrame basierend auf den gewünschten Titeln für einen bestimmten Level des Multiheaders.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, der gefiltert werden soll.\n",
    "            titles (list): Eine Liste von Strings, die die gewünschten Titel für den Filter enthalten.\n",
    "            level (int): Der Level des Multiheaders, auf dem der Filter angewendet werden soll.\n",
    "        Returns:\n",
    "            DataFrame: Der gefilterte DataFrame, der nur die Spalten mit den gewünschten Titeln enthält.\n",
    "        \"\"\"\n",
    "        # Create a mask for the desired subtitles for all columns in the DataFrame\n",
    "        column_mask = df.columns.isin(titles, level=level)\n",
    "        # Use the mask to filter the columns of the DataFrame\n",
    "        filtered_df = df.loc[:, column_mask]\n",
    "        return filtered_df\n",
    "\n",
    "    def filter_by_titles_or_subtitel(self,df,titles=[],subtitles=[],multheader =True):\n",
    "        \"\"\"\n",
    "        Filtert den DataFrame basierend auf den gewünschten Titeln oder Untertiteln (subtitles) des Multiheaders.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, der gefiltert werden soll.\n",
    "            titles (list): Eine Liste von Strings, die die gewünschten Titel für den Filter enthalten.\n",
    "            subtitles (list): Eine Liste von Strings, die die gewünschten Untertitel für den Filter enthalten.\n",
    "            multheader (bool): Gibt an, ob der gefilterte DataFrame den Multiheader beibehalten soll.\n",
    "                True, wenn der Multiheader beibehalten werden soll, False, wenn der Multiheader entfernt werden soll.\n",
    "        Returns:\n",
    "            DataFrame: Der gefilterte DataFrame, der nur die Spalten mit den gewünschten Titeln oder Untertiteln enthält.\n",
    "                Der Multiheader kann optional beibehalten oder entfernt werden.\n",
    "        \"\"\"\n",
    "        df_t = self.filter_by_titles(df,titles,0)\n",
    "        df_s = self.filter_by_titles(df,subtitles,1)\n",
    "        filtered_df =  pd.merge(df_t, df_s, left_index=True, right_index=True)\n",
    "        if multheader:\n",
    "            return filtered_df\n",
    "        return filtered_df.droplevel(0, axis=1)\n",
    "    \n",
    "    def read_multiindex_exel (self,path_pre,file_name):\n",
    "        \"\"\"\n",
    "        Liest einen Excel-Datei mit einem Multiindex und gibt den entsprechenden DataFrame zurück.\n",
    "        Args:\n",
    "            path_pre (str): Der Pfad zur Excel-Datei.\n",
    "            file_name (str): Der Name der Excel-Datei.\n",
    "        Returns:\n",
    "            DataFrame: Der DataFrame, der aus der Excel-Datei mit einem Multiindex gelesen wurde.\n",
    "        \"\"\"\n",
    "        df = pd.read_excel(path_pre+file_name, \n",
    "                            header=[0,1], \n",
    "                            index_col=[0,1])\n",
    "        return df\n",
    "        \n",
    "    def drop_duplicated(self,df):\n",
    "        \"\"\"\n",
    "        Entfernt doppelte Indizes (Zeilen) im DataFrame und gibt den bereinigten DataFrame zurück.\n",
    "        Args:\n",
    "            df (DataFrame): Der DataFrame, bei dem doppelte Indizes entfernt werden sollen.\n",
    "        Returns:\n",
    "            DataFrame: Der bereinigte DataFrame ohne doppelte Indizes.\n",
    "        \"\"\"\n",
    "        return df[~df.index.duplicated(keep='first')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameMerger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Klasse, die verschiedene Methoden zur Zusammenführung und Bearbeitung von DataFrames bereitstellt. Die Klasse erweitert die `MultindexDf`-Klasse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methoden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dfs_concat_header(df, dfs, header_name)`\n",
    "Fügt den angegebenen DataFrames den Header `header_name` hinzu und konkateniert sie entlang der Spaltenachse. Der Multiindex wird beibehalten.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, dem der Header hinzugefügt werden soll.\n",
    "- `dfs (dict)`: Ein Dictionary von DataFrames, die konkateniert werden sollen.\n",
    "- `header_name (str)`: Der Name des Headers, der den DataFrames hinzugefügt werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das aktualisierte Dictionary mit den konkatenierten DataFrames.\n",
    "\n",
    "#### `save_df_to_exel(df, path, file_name, file_end='.xlsx', index=False)`\n",
    "Speichert den DataFrame in einer Excel-Datei.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, der gespeichert werden soll.\n",
    "- `path (str)`: Der Pfad, in dem die Datei gespeichert werden soll.\n",
    "- `file_name (str)`: Der Name der Datei.\n",
    "- `file_end (str)`: Die Dateiendung (Standardwert: '.xlsx').\n",
    "- `index (bool)`: Gibt an, ob der Index in die Excel-Datei geschrieben werden soll (Standardwert: False).\n",
    "\n",
    "#### `save_df_dict_to_exel(dfs, path, file_end='.xlsx')`\n",
    "Speichert die DataFrames in einem Dictionary in separaten Excel-Dateien.\n",
    "\n",
    "- `dfs (dict)`: Ein Dictionary von DataFrames, die gespeichert werden sollen.\n",
    "- `path (str)`: Der Pfad, in dem die Dateien gespeichert werden sollen.\n",
    "- `file_end (str)`: Die Dateiendung für alle Dateien (Standardwert: '.xlsx').\n",
    "\n",
    "#### `merge_2_dict_of_df(dict_merge, dict_new)`\n",
    "Vereint zwei Dictionarys von DataFrames. Wenn die Schlüssel bereits vorhanden sind, werden die DataFrames zusammengeführt, andernfalls werden sie hinzugefügt.\n",
    "\n",
    "- `dict_merge (dict)`: Das erste Dictionary von DataFrames.\n",
    "- `dict_new (dict)`: Das zweite Dictionary von DataFrames.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das vereinigte Dictionary von DataFrames.\n",
    "\n",
    "#### `merge_if_not_identical(df1, df2)`\n",
    "Vereint zwei DataFrames, wenn sie nicht identisch sind.\n",
    "\n",
    "- `df1 (DataFrame)`: Das erste DataFrame.\n",
    "- `df2 (DataFrame)`: Das zweite DataFrame.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Das vereinigte DataFrame, wenn die DataFrames nicht identisch sind, andernfalls das erste DataFrame.\n",
    "\n",
    "#### `dict_merge_with_multiheader(dfs, on=None, how='left')`\n",
    "Vereint die DataFrames in einem Dictionary und erstellt einen Multiindex für die Spaltenköpfe.\n",
    "\n",
    "- `dfs (dict)`: Ein Dictionary von DataFrames, die vereint werden sollen.\n",
    "- `on (str oder list)`: Die Spalten oder der Spaltenname, auf denen die DataFrames vereint werden sollen (Standardwert: None).\n",
    "- `how (str)`: Der Vereinigungstyp ('left', 'right', 'inner', 'outer', Standardwert: 'left').\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Das vereinigte DataFrame mit einem Multiindex für die Spaltenköpfe.\n",
    "\n",
    "#### `rename_col_from_df(df, colname='Column')`\n",
    "Benennt die Spalten eines DataFrames um und gibt eine Zuordnungstabelle zurück.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, dessen Spalten umbenannt werden sollen.\n",
    "- `colname (str)`: Der Präfix für die neuen Spaltennamen (Standardwert: 'Column').\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit umbenannten Spalten.\n",
    "- `DataFrame`: Eine Zuordnungstabelle, die die Original- und generierten Spaltennamen enthält.\n",
    "\n",
    "#### `rename_col_from_dict(df_dict)`\n",
    "Benennt die Spalten in einem Dictionary von DataFrames um und gibt eine Zuordnungstabelle zurück.\n",
    "\n",
    "- `df_dict (dict)`: Ein Dictionary von DataFrames, deren Spalten umbenannt werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das aktualisierte Dictionary von DataFrames.\n",
    "- `DataFrame`: Eine Zuordnungstabelle, die die Original- und generierten Spaltennamen enthält.\n",
    "\n",
    "#### `convert_index_to_int(df)`\n",
    "Konvertiert die Indexspalten eines DataFrames mit Multiindex in Ganzzahltypen.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame mit Multiindex.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit konvertierten Indexspalten.\n",
    "\n",
    "#### `convert_index_from_dict_df_to_int(dict_df)`\n",
    "Konvertiert die Indexspalten aller DataFrames in einem Dictionary von DataFrames in Ganzzahltypen.\n",
    "\n",
    "- `dict_df (dict)`: Ein Dictionary von DataFrames.\n",
    "\n",
    "#### `create_partial_duplicates(dict_obj)`\n",
    "Erstellt Teil-Duplikate von Schlüsseln in einem Dictionary, wenn ein Schlüssel zu 50% übereinstimmt.\n",
    "\n",
    "- `dict_obj (dict)`: Ein Dictionary-Objekt.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `list`: Eine Liste von Teil-Duplikaten von Schlüsseln im Dictionary.\n",
    "\n",
    "#### `name_change(col_names)`\n",
    "Ändert die Namen der Spalten in einer Liste von Spaltennamen.\n",
    "\n",
    "- `col_names (list)`: Eine Liste von Spaltennamen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `list`: Eine Liste von Spaltennamen mit geänderten Namen.\n",
    "\n",
    "#### `merge_same_df_form_dict(dict_df, duplicates_df)`\n",
    "Vereint DataFrames mit identischen Spaltennamen in einem Dictionary von DataFrames.\n",
    "\n",
    "- `dict_df (dict)`: Ein Dictionary von DataFrames.\n",
    "- `duplicates_df (list)`: Eine Liste von Duplikaten von DataFrames mit identischen Spaltennamen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das aktualisierte Dictionary von DataFrames.\n",
    "\n",
    "#### `replace_symbols(dict_df)`\n",
    "Ersetzt Symbole in den DataFrames eines Dictionarys durch numerische Werte.\n",
    "\n",
    "- `dict_df (dict)`: Ein Dictionary von DataFrames.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `dict`: Das aktualisierte Dictionary von DataFrames.\n",
    "\n",
    "#### `fill_df(df)`\n",
    "Füllt NaN-Werte in einem DataFrame durch Vorwärts- und Rückwärtsfüllung.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, der gefüllt werden soll.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der gefüllte DataFrame.\n",
    "\n",
    "#### `get_columns_with_nan(df)`\n",
    "Gibt eine Liste der Spaltennamen zurück, die NaN-Werte enthalten.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, dessen Spalten überprüft werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `list`: Eine Liste der Spaltennamen mit NaN-Werten.\n",
    "\n",
    "#### `drop_Nan_col(df)`\n",
    "Entfernt Spalten aus einem DataFrame, die ausschließlich NaN-Werte enthalten.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, aus dem Spalten entfernt werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit entfernten Spalten.\n",
    "\n",
    "#### `check_string_length(str1, str2)`\n",
    "Überprüft, ob die Länge von zwei Strings eine maximale Abweichung von 3 Buchstaben aufweist.\n",
    "\n",
    "- `str1 (str)`: Der erste String.\n",
    "- `str2 (str)`: Der zweite String.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `bool`: True, wenn die Längen der Strings eine maximale Abweichung von 3 Buchstaben aufweisen, sonst False.\n",
    "\n",
    "#### `create_multiindex(df, index_col)`\n",
    "Erstellt einen Multiindex für den angegebenen DataFrame.\n",
    "\n",
    "- `df (DataFrame)`: Der DataFrame, für den der Multiindex erstellt werden soll.\n",
    "- `index_col (str oder list)`: Der Name oder die Namen der Spalten, die als Index verwendet werden sollen.\n",
    "\n",
    "**Rückgabewert**\n",
    "- `DataFrame`: Der DataFrame mit Multiindex.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameMerger(MultindexDf):\n",
    "    def __init__(self):\n",
    "        pass  \n",
    "    \n",
    "    def dfs_concat_header(self, df, dfs, header_name):\n",
    "        for k in dfs:            \n",
    "            merged = pd.concat([df[header_name], dfs[k]], axis=1, join='outer')\n",
    "            merged = self.create_multiindex(merged,self.index_col)\n",
    "            dfs[k] = merged\n",
    "        return dfs      \n",
    "    \n",
    "    def save_df_to_exel(self,df,path,file_name,file_end='.xlsx',index=False):\n",
    "        if self.df_has_Multindex(df):\n",
    "            index = True\n",
    "        df.to_excel(path+file_name+file_end, index=index)\n",
    "    \n",
    "    def save_df_dict_to_exel(self,dfs,path,file_end='.xlsx'):\n",
    "        for name, df in dfs.items():            \n",
    "            self.save_df_to_exel(df,path,name,file_end)\n",
    "            \n",
    "    \n",
    "    def merge_2_dict_of_df(self,dict_merge,dict_new):\n",
    "        if not dict_merge:\n",
    "            return dict_new\n",
    "        for key in dict_new.keys():\n",
    "            if key not in dict_merge:\n",
    "                dict_merge[key] = dict_new[key]\n",
    "            else:\n",
    "                dict_merge[key] = self.merge_if_not_identical(dict_merge[key],dict_new[key])\n",
    "            dict_merge[key] = dict_merge[key].drop_duplicates(subset=None)\n",
    "        \n",
    "        return dict_merge\n",
    "\n",
    "    def merge_if_not_identical(self,df1, df2):\n",
    "        if not df1.equals(df2):\n",
    "            merged_df = pd.concat([df1, df2])\n",
    "            return merged_df\n",
    "        else:\n",
    "            return df1\n",
    "        \n",
    "    def dict_merge_with_multiheader(self,dfs, on=None, how='left'):\n",
    "        # Merge die DataFrames und erstelle einen Multi-Index als Spaltenköpfe\n",
    "        merged_df = None\n",
    "        for key, df in dfs.items():\n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = pd.merge(merged_df, df,left_index=True, right_index=True, on=on, how=how, suffixes=('', f'_{key}'))\n",
    "        return merged_df\n",
    "        \n",
    "    def rename_col_from_df(self,df,colname='Column'):\n",
    "        original_columns = []\n",
    "        new_columns = []\n",
    "\n",
    "        for i, column in enumerate(df.columns):\n",
    "            new_column = f'{colname}_{i}'\n",
    "            new_columns.append(new_column)\n",
    "            original_columns.append(column)\n",
    "        \n",
    "        df.columns = new_columns\n",
    "        mapping_df = pd.DataFrame({'Original': original_columns, 'Generated': new_columns})\n",
    "        return df , mapping_df\n",
    "\n",
    "    def rename_col_from_dict (self,df_dict):\n",
    "        l_mapping_df = []\n",
    "        for name, df in df_dict.items():\n",
    "            if self.df_has_Multindex(df):\n",
    "                df, mapping_df = self.rename_col_from_df(df,name)\n",
    "                l_mapping_df.append(mapping_df)\n",
    "        try:\n",
    "            mapping_df  = pd.concat(l_mapping_df)\n",
    "        except ValueError as e:\n",
    "            print(f'{e}')\n",
    "            mapping_df = []\n",
    "        return df_dict , mapping_df\n",
    "\n",
    "    def convert_index_to_int(self,df):\n",
    "        \"\"\"\n",
    "        Diese Methode wandelt die Index-Spalten eines Multiindex-Datenrahmens in Ganzzahltypen um.\n",
    "        \n",
    "        :param df: Der Multiindex-Datenrahmen.\n",
    "        :return: Der Multiindex-Datenrahmen mit Integer-Index-Spalten.\n",
    "        \"\"\"\n",
    "        # Erhalte den aktuellen Index des DataFrames\n",
    "        index = df.index \n",
    "\n",
    "        \n",
    "        # Konvertiere die Index-Spalten in Ganzzahltypen\n",
    "        new_index = pd.MultiIndex.from_tuples([tuple(map(int, t)) for t in index], names=index.names)\n",
    "        \n",
    "        # Ersetze den alten Index durch den neuen Index\n",
    "        df.index = new_index\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def convert_index_from_dict_df_to_int(self,dict_df):\n",
    "        for _, df in dict_df.items():\n",
    "            self.convert_index_to_int(df)\n",
    "\n",
    "    def check_string_length(self,str1, str2):\n",
    "        \"\"\"\n",
    "        Überprüft, ob die Längen von zwei Strings eine maximale Abweichung von 3 Buchstaben haben.\n",
    "        :param str1: Erster String\n",
    "        :param str2: Zweiter String\n",
    "        :return: True, wenn die Längen der Strings eine maximale Abweichung von 3 Buchstaben haben, andernfalls False.\n",
    "        \"\"\"\n",
    "        length_diff = abs(len(str1) - len(str2))  # Berechnet den Unterschied in der Länge der Strings\n",
    "        if length_diff <= 3:  # Überprüft, ob die maximale Abweichung von 3 Buchstaben überschritten wird\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def create_partial_duplicates(self,dict_obj):\n",
    "        \"\"\"\n",
    "        Erstellt Teil-Duplikate von Dictionary-Keys, wenn ein Schlüssel zu 50% übereinstimmt.\n",
    "        :param dict_obj: Ein Dictionary-Objekt\n",
    "        :return: Eine Liste von Teil-Duplikaten von Dictionary-Keys\n",
    "        \"\"\"\n",
    "        all_keys = dict_obj.keys()  # Extrahiert alle Schlüssel aus dem Dictionary-Objekt\n",
    "        partial_duplicates = []\n",
    "        for i, key1 in enumerate(all_keys):\n",
    "            for j, key2 in enumerate(all_keys):\n",
    "                if i != j and key1 in key2 and  self.check_string_length(key1,key2):\n",
    "                    partial_duplicates.append((key1,key2))  # Fügt Teil-Duplikat hinzu\n",
    "\n",
    "        return sorted(list(set(partial_duplicates)))\n",
    "\n",
    "    def name_change(self,col_names):\n",
    "        temp=''\n",
    "        new_col = []\n",
    "        for s in col_names:\n",
    "            stat_s, end_s = s.split('_',1)   \n",
    "            \n",
    "            if len(stat_s) < 4:\n",
    "                new_name = f'{temp}_{end_s}'\n",
    "                new_col.append(new_name)\n",
    "            else:\n",
    "                temp = stat_s\n",
    "                new_col.append(s)\n",
    "        return new_col\n",
    "    \n",
    "    def merge_same_df_form_dict(self,dict_df,duplicates_df):\n",
    "        for name_df_1,name_df_2 in duplicates_df:\n",
    "            col_name = self.name_change(dict_df[name_df_1].columns)\n",
    "            dict_df[name_df_1].columns = col_name\n",
    "            dict_df[name_df_2].columns = col_name\n",
    "            merged_df = pd.concat([dict_df[name_df_1],dict_df[name_df_2]],axis=0)\n",
    "            dict_df[name_df_1] = merged_df\n",
    "            del dict_df[name_df_2]\n",
    "        return dict_df\n",
    "\n",
    "    def replace_symbols(self,dict_df):\n",
    "        for name, df in dict_df.items():  \n",
    "            df = df.apply(pd.to_numeric, errors='coerce')#.astype('float64')\n",
    "            dict_df[name] = df\n",
    "        return dict_df \n",
    "    \n",
    "    def fill_df (self,df):\n",
    "        #df = df.groupby(level=0, group_keys=False).apply(lambda x: x.fillna(method='ffill'))\n",
    "        #df = df.groupby(level=0, group_keys=False).apply(lambda x: x.fillna(method='bfill'))\n",
    "        df =df.fillna(df.groupby(level=0).ffill())\n",
    "        df = df.fillna(df.groupby(level=0).bfill())\n",
    "        return df\n",
    "    def get_columns_with_nan(self,df):\n",
    "        nan_columns = []\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any():\n",
    "                nan_columns.append(col)\n",
    "        return nan_columns\n",
    "\n",
    "    def drop_Nan_col(self,df):\n",
    "        return df.dropna(axis=1, how='all')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExcelProcessor\n",
    "\n",
    "Eine Klasse zur Verarbeitung von Excel-Dateien.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `path_raw` (str): Der Pfad zum Verzeichnis, in dem die Excel-Datei liegt.\n",
    "- `file_name` (str): Der Name der Excel-Datei.\n",
    "- `drop_col_name` (list): Eine Liste von Spaltennamen, die ausgelassen werden sollen. Standardmäßig sind 'Veränderung_in_' und 'Gemeindename' ausgeschlossen.\n",
    "- `header_name` (str): Der Name der Hauptspalte, die die Gemeindenamen enthält. Standardmäßig ist 'Gemeinden'.\n",
    "- `index_col` (list): Eine Liste von Spaltennamen, die als Index verwendet werden sollen. Standardmäßig sind ['Gemeindecode', 'Jahr'].\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- `path_raw` (str): Der Pfad zum Verzeichnis, in dem die Excel-Datei liegt.\n",
    "- `file_name` (str): Der Name der Excel-Datei.\n",
    "- `index_col` (list): Eine Liste von Spaltennamen, die als Index verwendet werden sollen.\n",
    "- `df` (DataFrame): Der geladene DataFrame aus der Excel-Datei.\n",
    "- `header_list_1` (list): Die Liste der Spaltennamen der ersten Header-Ebene.\n",
    "- `header_list_2` (list): Die Liste der Spaltennamen der zweiten Header-Ebene.\n",
    "- `header_name` (str): Der Name der Hauptspalte, die die Gemeindenamen enthält.\n",
    "- `drop_col_names` (list): Eine Liste von Spaltennamen, die ausgelassen werden sollen.\n",
    "- `dfs` (dict): Ein Wörterbuch mit den verbundenen DataFrames für jedes Jahr.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methoden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### `load_data()`\n",
    "\n",
    "Lädt die Daten aus der Excel-Datei.\n",
    "\n",
    "##### `process_data()`\n",
    "\n",
    "Verarbeitet die geladenen Daten.\n",
    "\n",
    "##### `drop_columns()`\n",
    "\n",
    "Entfernt die angegebenen Spalten aus dem DataFrame.\n",
    "\n",
    "##### `rename_header(header_list, start_header='Gemeinden')`\n",
    "\n",
    "Benennt die Header-Einträge um und gibt die Liste der Header zurück.\n",
    "\n",
    "##### `replace_space(header_list)`\n",
    "\n",
    "Ersetzt Leerzeichen in den Header-Namen durch Unterstriche.\n",
    "\n",
    "##### `connect_jahr(df, jahr)`\n",
    "\n",
    "Verbindet die Daten für jedes Jahr und gibt ein Wörterbuch mit den verbundenen DataFrames zurück.\n",
    "\n",
    "##### `get_df()`\n",
    "\n",
    "Gibt den geladenen DataFrame zurück.\n",
    "\n",
    "##### `get_columns_name()`\n",
    "\n",
    "Gibt eine Liste der Spaltennamen des geladenen DataFrames zurück.\n",
    "\n",
    "##### `get_dfs()`\n",
    "\n",
    "Gibt das Wörterbuch mit den verbundenen DataFrames zurück."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExelProcessor(DataFrameMerger):\n",
    "    def __init__(self, path_raw, file_name,drop_col_name=['Veränderung_in_','Gemeindename'] ,header_name = 'Gemeinden',index_col=['Gemeindecode', 'Jahr']):\n",
    "        self.path_raw = path_raw\n",
    "        self.file_name = file_name\n",
    "        self.index_col =index_col\n",
    "        \n",
    "        self.df = None\n",
    "        self.header_list_1 = None\n",
    "        self.header_list_2 = None\n",
    "        self.header_name = header_name\n",
    "        self.drop_col_names= drop_col_name\n",
    "        self.dfs = {}     \n",
    "        \n",
    "        self.load_data()\n",
    "        self.process_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        # Load data from file\n",
    "        self.df = pd.read_excel(self.path_raw + self.file_name, skiprows=3)\n",
    "        \n",
    "    def process_data(self):\n",
    "        # Rename columns\n",
    "        self.header_list_1 = self.df.columns.to_list()\n",
    "        self.header_list_1 = self.rename_header(self.header_list_1)\n",
    "        self.header_list_1 = self.replace_space(self.header_list_1)\n",
    "\n",
    "        self.header_list_2 = self.df.iloc[1].to_list()\n",
    "        #print (header_list_2)\n",
    "        self.header_list_2 = self.replace_space(self.header_list_2)\n",
    "       \n",
    "        # Create multi-level headers\n",
    "        self.df = self.create_multiheader(self.df, self.header_list_1, self.header_list_2)\n",
    "        self.df = self.drop_colmns()\n",
    "        \n",
    "       \n",
    "        # Connect data for each year\n",
    "        jahr = self.df.iloc[2]\n",
    "        jahr = jahr.to_frame()\n",
    "        \n",
    "        \n",
    "         # Drop NaN rows and reset index\n",
    "        self.df = self.df.dropna()[1:].reset_index(drop=True)\n",
    "        self.dfs = self.conect_jahr(self.df, jahr)\n",
    "\n",
    "        # Merge Gemeinden column with each DataFrame\n",
    "        self.dfs = self.dfs_concat_header(self.df, self.dfs, self.header_name)\n",
    "\n",
    "    def drop_colmns(self):\n",
    "        for col in self.drop_col_names:\n",
    "            self.df.drop(columns=self.df.filter(regex=col).columns, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def rename_header(self, header_list, start_header='Gemeinden'):\n",
    "        header_list[0] = start_header\n",
    "        \n",
    "        for t in range(len(header_list)):\n",
    "            if 'Unnamed' in header_list[t]:\n",
    "                header_list[t] = title\n",
    "            else:\n",
    "                title = header_list[t]\n",
    "        return header_list\n",
    "\n",
    "    def replace_space(self, header_list):\n",
    "        header_list = [l.replace(' ', '_') for l in header_list]\n",
    "        return header_list\n",
    "\n",
    "\n",
    "    def conect_jahr(self, df, jahr):\n",
    "        heder_level_0 = df.columns.get_level_values(0).unique()\n",
    "        for col_name in heder_level_0[1:]:\n",
    "            col_j = jahr.loc[col_name][2].to_list()\n",
    "            if len(set(col_j)) == 1:\n",
    "                df_tmp = df[col_name].reset_index(drop=True)\n",
    "                if type(col_j[0])==str:\n",
    "                    jar = int(col_j[0][:4])\n",
    "                else:\n",
    "                    jar = col_j[0]\n",
    "                df_tmp['Jahr'] = np.full(len(df_tmp), jar)            \n",
    "                self.dfs[col_name] = df_tmp\n",
    "        return self.dfs\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def get_colmns_name(self):\n",
    "        return self.df.columns.to_list()\n",
    "     \n",
    "    def get_dfs(self):\n",
    "        return self.dfs\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataQualityAssessment\n",
    "Die Klasse ```DataQualityAssessment``` erbt von der Klasse ```DataFrameMerger``` und bietet Methoden zur Durchführung einer Datenqualitätsbewertung. Die Klasse verfügt über eine ```__init__```-Methode zum Initialisieren des Objekts mit einem DataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vererbung\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `DataQualityAssessment`-Klasse erbt von der `DataFrameMerger`-Klasse.\n",
    "\n",
    "#### Konstruktor\n",
    "\n",
    "##### `__init__(self, df)`\n",
    "\n",
    "Erstellt eine neue Instanz der `DataQualityAssessment`-Klasse mit dem gegebenen DataFrame (df)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methoden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### `check_missing_values()`\n",
    "\n",
    "Überprüft den gegebenen DataFrame auf fehlende Werte. Fehlende Werte werden aufgefüllt, und die Spalten mit fehlenden Werten werden ausgegeben. Die Methode ruft sich selbst rekursiv auf, um sicherzustellen, dass alle fehlenden Werte behandelt werden.\n",
    "\n",
    "##### `check_data_type()`\n",
    "\n",
    "Überprüft die Datentypen der Spalten im gegebenen DataFrame und gibt sie aus.\n",
    "\n",
    "##### `check_duplicates()`\n",
    "\n",
    "Überprüft den gegebenen DataFrame auf Duplikate. Wenn Duplikate gefunden werden, werden sie entfernt.\n",
    "\n",
    "##### `run_dqa()`\n",
    "\n",
    "Führt eine umfassende Datenqualitätsbewertung durch. Die Methode ruft die Methoden `check_missing_values()`, `check_data_type()` und `check_duplicates()` auf und gibt den bereinigten DataFrame sowie Informationen über fehlende Werte zurück.\n",
    "\n",
    "**Rückgabewerte**\n",
    "\n",
    "Die Methode `run_dqa()` gibt den bereinigten DataFrame und eine Liste von fehlenden Werten zurück."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityAssessment(DataFrameMerger):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        pass\n",
    "    def check_missing_values(self):\n",
    "        #self.df = self.fill_df(self.df)\n",
    "        self.df = self.fill_df(self.df)\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        if missing_values.sum() == 0:\n",
    "            print(\"Keine fehlenden Werte gefunden.\")\n",
    "            return \n",
    "        \n",
    "        print(\"Folgende Spalten haben fehlende Werte:\")\n",
    "        print(missing_values[missing_values != 0])       \n",
    "        self.df = self.df.dropna(axis=1,how='all')\n",
    "        self.df = self.df.fillna(value=0)\n",
    "        self.check_missing_values()\n",
    "       \n",
    "        return missing_values[missing_values != 0]\n",
    "    \n",
    "    def check_data_type(self):\n",
    "        nummers = ['']\n",
    "        data_types = self.df.dtypes\n",
    "        print(\"Daten-Typen:\")\n",
    "        print(data_types.to_list())\n",
    "    \n",
    "    def check_duplicates(self):\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        if duplicates == 0:\n",
    "            print(\"Keine Duplikate gefunden.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Es wurden {} Duplikate gelöscht.\".format(duplicates))\n",
    "        self.df = self.drop_duplicated(self.df)\n",
    "        \n",
    "    \n",
    "    def run_dqa(self):\n",
    "        missing_value = self.check_missing_values()\n",
    "        print('Data Types')\n",
    "        self.check_data_type()\n",
    "        print('Data duplicats')\n",
    "        duplicates = self.check_duplicates()\n",
    "\n",
    "        return self.df,missing_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PXProcessor\n",
    "Die Klasse `PXProcessor` ermöglicht das Verarbeiten von `.px`-Dateien mithilfe der `pyaxis` Bibliothek."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `path` (str): Der Pfad zum Verzeichnis, das die `.px`-Dateien enthält."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `path` (str): Der Pfad zum Verzeichnis der `.px`-Dateien.\n",
    "- `px_end` (str): Die Dateiendung der `.px`-Dateien.\n",
    "- `df_dict_all` (dict): Ein Wörterbuch, das alle DataFrames und Metadaten der `.px`-Dateien enthält.\n",
    "- `df_dict` (dict): Ein Wörterbuch, das nur die verarbeiteten DataFrames der `.px`-Dateien enthält."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methoden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `process_px_files(self)`: Liest und verarbeitet die `.px`-Dateien im angegebenen Verzeichnis.\n",
    "- `get_column_names_except(self, df, excluded_columns)`: Gibt eine Liste der Spaltennamen des DataFrames zurück, ausgenommen der ausgeschlossenen Spalten.\n",
    "- `remove_rows_by_values(self, df, column, values)`: Entfernt Zeilen aus dem DataFrame, die bestimmte Werte in der angegebenen Spalte enthalten.\n",
    "- `split_col_px(self, df)`: Teilt den DataFrame auf, um die Spalten 'Gemeindecode' und 'Gemeindename' zu extrahieren und den DataFrame entsprechend neu zu organisieren.\n",
    "- `process_dataframes(self)`: Verarbeitet die DataFrames der `.px`-Dateien und speichert sie im `df_dict`.\n",
    "- `get_dataframe(self, name)`: Gibt das DataFrame mit dem angegebenen Namen aus dem `df_dict` zurück.\n",
    "- `get_df_dict(self)`: Gibt das `df_dict` zurück, das alle verarbeiteten DataFrames enthält."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaxis import pyaxis\n",
    "class PXProcessor:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.px_end = '.px'\n",
    "        self.df_dict_all= {}\n",
    "        self.df_dict = {}\n",
    "        self.process_px_files()\n",
    "        self.process_dataframes()\n",
    "    \n",
    "    def process_px_files(self):\n",
    "        files = os.listdir(self.path)\n",
    "        px_files_name = [f.replace('.px', '') for f in files if f.endswith('.px')]\n",
    "        \n",
    "        for name in px_files_name:\n",
    "            px = pyaxis.parse(self.path + name + self.px_end, encoding='ISO-8859-15')\n",
    "            self.df_dict_all[name] = (px['DATA'], px['METADATA'])\n",
    "    \n",
    "    def get_column_names_except(self, df, excluded_columns):\n",
    "        all_columns = df.columns.tolist()\n",
    "        included_columns = [col for col in all_columns if col not in excluded_columns]\n",
    "        return included_columns\n",
    "\n",
    "    def remove_rows_by_values(self, df, column, values):\n",
    "        return df[~df[column].isin(values)]\n",
    "\n",
    "    def split_col_px(self, df):\n",
    "        if 'Jahr' in df.columns:\n",
    "            col_mask = df.columns.str.contains('Gemeinde (......)', case=False, regex=False)\n",
    "\n",
    "            if not (any(col_mask) or ('Gemeinde' in df.columns)):\n",
    "                return None\n",
    "\n",
    "            if any(col_mask):\n",
    "                # Auswahl der Spalten basierend auf der Liste der Booleschen Werte\n",
    "                selected_cols = df.iloc[:, col_mask]\n",
    "                # Anwenden der `startswith()`-Methode auf jede Spalte\n",
    "                mapping = selected_cols.apply(lambda x: x.str.startswith('.....'))\n",
    "                # Entfernen von unerwünschten Zeichen aus der ausgewählten Spalte\n",
    "                gemeinde_col = selected_cols.iloc[:, 0].str.replace('\\.+', '', regex=True)\n",
    "                gemeinde_col = gemeinde_col.str.split(' ', n=1, expand=True)\n",
    "                df[['Gemeindecode', 'Gemeindename']] = gemeinde_col\n",
    "                df = df.drop(selected_cols.columns.to_list(), axis=1)\n",
    "\n",
    "            if 'Gemeinde' in df.columns:\n",
    "                df[['Gemeindecode', 'Gemeindename']] = df['Gemeinde'].str.split(' ', n=1, expand=True)\n",
    "                df.drop(['Gemeinde'], inplace=True, axis=1)\n",
    "\n",
    "            df.dropna(inplace=True)\n",
    "            df = self.remove_rows_by_values(df, 'Gemeindecode', ['<<', '-'])\n",
    "            df['Gemeindecode'] = df['Gemeindecode'].astype(int)\n",
    "            df['Jahr'] = df['Jahr'].astype(int)\n",
    "            flip_col = self.get_column_names_except(df, ['Gemeindecode', 'Jahr', 'Gemeindename', 'DATA'])\n",
    "            df = df.pivot(index=['Gemeindecode', 'Jahr'], columns=flip_col, values='DATA')\n",
    "            return df\n",
    "\n",
    "    def process_dataframes(self):\n",
    "        for key, (df, meta) in self.df_dict_all.items():\n",
    "            df = self.split_col_px(df)\n",
    "            if df is not None:\n",
    "                self.df_dict[key] = df\n",
    "\n",
    "    def get_dataframe(self, name):\n",
    "        return self.df_dict[name] if name in self.df_dict else None\n",
    "    \n",
    "    def get_df_dict(self):\n",
    "        return self.df_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Verarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = '.\\Raw\\Kennzahlen_aller_Gemeinden\\\\'\n",
    "path_pre = '.\\Preparation\\Kennzahlen_aller_Gemeinden\\\\'\n",
    "path_px = '.\\Raw\\Kennzahlen_aller_Gemeinden\\PX\\\\'\n",
    "path_gemeindeverzeichnis = '.\\Raw\\Kennzahlen_aller_Gemeinden\\Gemeindeverzeichnis\\Gemeindeverzeichnis.xlsx'\n",
    "path_luzern = '.\\Preparation\\merged_typologien.xlsx'\n",
    "\n",
    "index_col= ['Gemeindecode', 'Jahr']\n",
    "\n",
    "\n",
    "dfm = DataFrameMerger()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Gemeinden und Grossregionen der Schweiz wurden in den Daten erfasst. Da diese Arbeit sich auf den Kanton Luzern konzentrierte, wurden nur die Daten des Kantons Luzern verwendet. Dadurch wurde die Ausführung des Skripts beschleunigt. Es dauerte jedoch einige Minuten, um alle Daten einzulesen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exel einlesen und verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path_raw)\n",
    "files_name = {'exel':[f for f in files if f.endswith('.xlsx') or f.endswith('.xls')]}\n",
    "exel_dict = { }\n",
    "\n",
    "for types in files_name:\n",
    "    for file_name in files_name[types]:\n",
    "        if types != 'exel':\n",
    "            raise ValueError (f'Das file ist keine Exel datei {file_name}')\n",
    "        \n",
    "        dfp = ExelProcessor(path_raw, file_name,index_col=index_col)\n",
    "        dfs = dfp.get_dfs()\n",
    "        \n",
    "        exel_dict = dfm.merge_2_dict_of_df(exel_dict,dfs)\n",
    "\n",
    "    \n",
    "\n",
    "try:\n",
    "    del exel_dict['Fläche_1)']\n",
    "except KeyError as e:\n",
    "    print (e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PxDaten einlesen und verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = PXProcessor(path_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_dict = PP.get_df_dict()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemeindeverzeichnis einlesen und verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_gemindeverzeichnis = pd.read_excel(path_gemeindeverzeichnis,sheet_name='GDE' ).dropna()\n",
    "df_luzern = pd.read_excel(path_luzern).dropna()\n",
    "\n",
    "mapping_luzern= {'Gemeinde ID':'Gemeindecode','Gemeinde':'Gemeindename'}\n",
    "df_luzern= df_luzern.rename(columns=mapping_luzern)\n",
    "\n",
    "mapping_gemindeverzeichnis= {'GDENR':'Gemeindecode','GDENAME':'Gemeindename'}\n",
    "df_gemindeverzeichnis= df_gemindeverzeichnis.rename(columns=mapping_gemindeverzeichnis)\n",
    "df_gemindeverzeichnis = df_gemindeverzeichnis.drop(columns=['GDENAMK','GDEMUTDAT'])\n",
    "\n",
    "mapping_merge_luzern_gemindeverzeichnis = {'Gemeindename_y':'Gemeindename'}\n",
    "df_merge_luzern_gemindeverzeichnis = pd.merge(df_gemindeverzeichnis,df_luzern,on=['Gemeindecode'])\n",
    "df_merge_luzern_gemindeverzeichnis= df_merge_luzern_gemindeverzeichnis.rename(columns=mapping_merge_luzern_gemindeverzeichnis)\n",
    "df_merge_luzern_gemindeverzeichnis = df_merge_luzern_gemindeverzeichnis.drop(columns=['Gemeindename_x'])\n",
    "\n",
    "df_merge_luzern_gemindeverzeichnis = dfm.create_multiindex(df_merge_luzern_gemindeverzeichnis,index_col)\n",
    "df_merge_luzern_gemindeverzeichnis.loc[df_merge_luzern_gemindeverzeichnis['Gemeindetypologien']== 'Stadt','Gemeindetypologien'] = 'Kern'\n",
    "\n",
    "df_dict ={'Gemeindeinfo':df_merge_luzern_gemindeverzeichnis}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wurden die Daten in die gewünschte Struktur gebracht, insbesondere bei dem Datenset \"Bauausgaben und Arbeitsvorrat nach Grossregion, Kanton, Gemeinde,\n",
    "Art der Auftraggeber, Art der Bauwerke und Art der Arbeiten\". Dieser Datensatz hatte mehr Dimensionen in den Spaltennamen im Vergleich zu den anderen Datensets.\n",
    "Daher wurden die Spaltennamen angepasst. Die alten und neuen Namen wurden in einer Mapping-Tabelle gespeichert.\n",
    "Sobald alle Daten die richtige Struktur hatten, wurden sie zusammengeführt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict.update(exel_dict)\n",
    "df_dict.update(px_dict)\n",
    "\n",
    "duplicates_names = dfm.create_partial_duplicates(df_dict)\n",
    "exel_dict = dfm.merge_same_df_form_dict(df_dict,duplicates_names)\n",
    "exel_dict = dfm.replace_symbols(df_dict)\n",
    "\n",
    "df_dict, df_mapping = dfm.rename_col_from_dict(df_dict)\n",
    "dfm.convert_index_from_dict_df_to_int(df_dict)\n",
    "df_dict = dfm.set_keys_as_multiheader(df_dict)\n",
    "df_merged = dfm.dict_merge_with_multiheader(df_dict)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Nachdem die Daten verbunden waren, wurden Duplikate identifiziert und entfernt, da keine Duplikate erwünscht waren. Alle Spalten wurden als Objekte eingelesen, da einige Spalten Platzhalter-Symbole enthielten. Für das Training des Modells wurden jedoch Integer- oder Float-Werte benötigt. Daher wurden die Platzhalter mit \"NaN\" ersetzt und der Spaltentyp in \"Int64\" oder \"Float\" umgewandelt. \n",
    "\n",
    "Um ausreichend Datenpunkte für das Training und Testen des Modells zu haben, wurden Daten von 1991 bis 2022 gesammelt. Nicht für alle Daten gab es Einträge von jedem Jahr, beispielsweise für die Gemeindefläche wurde nur im Jahr 1992 und 2004 erfasst. Die fehlenden Werte wurden durch die Werte des nächsten Jahres aufgefüllt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folgende Spalten haben fehlende Werte:\n",
      "Gemeindeinfo               GDEKT                            2974\n",
      "                           GDEBZNA                          2974\n",
      "                           GDEKTNA                          2974\n",
      "                           Gemeindename                     2974\n",
      "                           Gemeindetypologien               2974\n",
      "                                                            ... \n",
      "Bauinvestitionen_Gemeinde  Bauinvestitionen_Gemeinde_139    2974\n",
      "                           Bauinvestitionen_Gemeinde_140    2974\n",
      "                           Bauinvestitionen_Gemeinde_141    2974\n",
      "                           Bauinvestitionen_Gemeinde_142    2974\n",
      "                           Bauinvestitionen_Gemeinde_143    2974\n",
      "Length: 175, dtype: int64\n",
      "Keine fehlenden Werte gefunden.\n",
      "Data Types\n",
      "Daten-Typen:\n",
      "[dtype('int64'), dtype('int64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]\n",
      "Data duplicats\n",
      "Es wurden 47 Duplikate gelöscht.\n"
     ]
    }
   ],
   "source": [
    "dqa = DataQualityAssessment(df_merged)\n",
    "df,m  = dqa.run_dqa()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Der neu erzeugte Datenset wurden in eine grosse .xlsx-Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.save_df_to_exel(df,path_pre,'Alle_Daten')\n",
    "dfm.save_df_to_exel(df_mapping,path_pre,'Col_mapping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
